{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing project related libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing network related variables\n",
    "VALIDATION_SIZE = 0.2 #20 % of the train set\n",
    "BATCH_SIZE = 20\n",
    "NUM_WORKERS = 0\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data\n",
    "train_data = datasets.MNIST('../data/', download=True, train=True, transform=transform)\n",
    "test_data = datasets.MNIST('../data/', download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into train & val using torch's SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(VALIDATION_SIZE * num_train))\n",
    "train_index, val_index = indices[split:], indices[:split] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(train_index)\n",
    "val_sampler = SubsetRandomSampler(val_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "val_set = DataLoader(train_data, sampler=val_sampler, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "test_set = DataLoader(test_data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([20, 1, 28, 28])\n",
      "labels shape: torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "#Get the shape of images & Labels for trainset\n",
    "images, labels = next(iter(train_set))\n",
    "print(f\"images shape: {images.shape}\\nlabels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_set : 2400\n",
      "shape of val_set : 600\n",
      "shape of test_set : 500\n"
     ]
    }
   ],
   "source": [
    "#Get the sample numbers fort train,val & test set\n",
    "print(f\"shape of train_set : {len(train_set)}\\nshape of val_set : {len(val_set)}\\nshape of test_set : {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Network architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- As we are dealing with MNIST claasification problems objects are more or less of similar size and hence\n",
    "1.Multilayer Perceptron architecture will do better\n",
    "2.We'll also see more complex cases were MLP will not be good choice and we'll start dealing with CNNs -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=264, bias=True)\n",
      "  (fc3): Linear(in_features=264, out_features=128, bias=True)\n",
      "  (fc4): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        #defining the fuly connected layers\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        self.fc2 = nn.Linear(512, 264)\n",
    "        self.fc3 = nn.Linear(264, 128)\n",
    "        self.fc4 = nn.Linear(128, 10)\n",
    "\n",
    "        #dropout\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        out = self.fc4(x)\n",
    "        return out\n",
    "\n",
    "    \n",
    "#initialize the NN\n",
    "network = Net()\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterian = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(params=network.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainig the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nevere ever forget to initialize the \"optimizer.zero_grad()\" before prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Epochs done: 0/10, Train loss: 1.4043731387332081\n",
      "Number of Epochs done: 1/10, Train loss: 0.438463756494845\n",
      "Number of Epochs done: 2/10, Train loss: 0.3099811168791105\n",
      "Number of Epochs done: 3/10, Train loss: 0.23990778061018014\n",
      "Number of Epochs done: 4/10, Train loss: 0.1921890571443752\n",
      "Number of Epochs done: 5/10, Train loss: 0.1625956611026777\n",
      "Number of Epochs done: 6/10, Train loss: 0.14017074929705511\n",
      "Number of Epochs done: 7/10, Train loss: 0.12267133021348854\n",
      "Number of Epochs done: 8/10, Train loss: 0.10998310694926962\n",
      "Number of Epochs done: 9/10, Train loss: 0.09671217351948144\n"
     ]
    }
   ],
   "source": [
    "for i in range(NUM_EPOCHS):\n",
    "    #monitoring training loss\n",
    "    train_loss =0\n",
    "    \n",
    "    for images, labels in train_set:\n",
    "        optimizer.zero_grad()\n",
    "        out = network.forward(images)\n",
    "        loss = criterian(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    print(f\"Number of Epochs done: {i}/{NUM_EPOCHS}, Train loss: {train_loss/len(train_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running model against the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 0.12192165999673307\n"
     ]
    }
   ],
   "source": [
    "#running model against the validation set\n",
    "with torch.no_grad():\n",
    "    val_loss = 0\n",
    "    for images, labels in val_set:\n",
    "        out = network.forward(images)\n",
    "        loss = criterian(out, labels)\n",
    "        val_loss += loss.item()\n",
    "    print(f\"validation loss: {val_loss/len(val_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label : 7\n",
      "Actual label: 7\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(test_set))\n",
    "_, pred = torch.max(network.forward(images[0]), 1)\n",
    "print(f\"Predicted label : {pred.item()}\\nActual label: {labels[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
